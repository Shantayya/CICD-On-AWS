CloudBees Jenkins 2.346.40.0.16   (release update every 2 month)

Jenkins open source automation server that is used to automate all sort of tasks related to building(maven), testing(Junit), delivering and even deploying the software.

Jenkins Plugins:
Jenkins functionality can be extended through plugins. Plugins provide additional features and integrations with various tools
Jenkins has plugin repository where you can download .hpi file and upload it your server. Manage jenkins --> manage plugins --> Advanced
or copy to plugin directory /var/lib/jenkins

Master-Slave architecture:
A Standalone Jenkins instance can grow quickly into disk-munching,cpu eating monster. To prevent this from happening we can scale Jenkins by implementing a slave node architecture,which can help us offload some of responsibilities of master jenkins instance. 

Jenkins master:
1. Holds all key configuration
2. Acts like controller or orchestrator
3. Sheduling and monitoring build tasks 
4. Dispatching build to slaves
5. Monitor the slaves etc

Jenkins slave:
1. Java executable that runs on remote machine
2. Listen to requests from master and executes build jobs
3. Slaves can run on variety of OS etc

static agents:			dynamic agents:
vm (nodes)			container or pods (cloud)

Note: Jenkins master node uses JNLP(Java network launch protocol) service on port 50000 to connect to slaves for dispatching jobs. /var/jenkins_home is deafult jenkins home when used container and /var/lib/jenkins/ if use jenkins as service. 

----Run Jenkins as docker container----
usermod -aG docker jenkins		--> adding jenkins user to docker group so that jenkins user can access docker. 
newgrp docker				--> reloading docker group	
docker volume create jenkins_home
docker run -d --name Jenkins -v jenkins_home:/var/jenkins_home -p 8080:8080 -p 50000:50000 --restart always jenkins/jenkins:lts-jks11

docker exec -it Jenkins cat /var/jenkins_home/secret/initialAdminPassword 		--> to see jenkins login password

---run Jenkins as service---
sudo yum update -y
sudo amazon-linux-extras install epel -y
sudo amazon-linux-extras install java-openjdk11 -y

add jenkins repo, install using yum and start system services 

logs: /var/log/jenkins/jenkins.log
configuration parameter available at: /etc/sysconfig/jenkins like to change JENKINS_HOME


----Jenkins job-----
Sequential set of tasks that user defines. ex: a Job can fetch source code from version control,build the code,run unit tests etc
job is synonymous with project. 
Jenkins support 2 types of jobs boradly:
1. Freestyle	--> default job type and are configured through the Jenkins web UI and are suitable for simple and straightforward tasks like running shell cmnds
2. Pipeline	--> series of build steps and maintained as code. 

number of job types depends on the plugin that you have installed. ex: maven project/job is only available if you installed the maven plugin. 
other job types include:
3. multibranch pipeline 			--> creates separate folder for each branch and creates job inside it. 
4. Folder					--> not a job type rather a way how you arrange jobs. 
5. Multiconfiguration project


--Env variables---
in Jenkins Pipeline, there are lot of env variables  that can  be accessed and maintained during build execution. can be seen at jenkins-url/env-vars.html
echo "JOB_NAME=${JOB_NAME}"

---Manage Jenkins--- (core configuration stored in config.xml under /var/lib/jenkins)
system configuration:
1. configure system (now system): configure gobal settings(email(normal and extended) notification, global properties like env var,tools locations etc,system msg to promulgate msg for jenkins users like maintainance,executors(number of vCPU's) shows number of jobs can run at a time, quite period(buffer time to kick off jobs),SCM checkout retry count,Jenkins URL, and paths (JDK).

2. global tool configuration (tools): configure the paths of the tools which you integrate with jenkins. provides auto installation also
3. Manage plugins: Jenkins functionality can be extended through plugins. Plugins provide additional features and integrations with various tools. Mostly Jenkins server will not opened to iternet so use plugin manager to configure proxy settings so that Jenkins can connect to internet to download Plugins. 

4. manage nodes and cloud: manage configuration of nodes (adding new node,configure their usage (number of executors,label), specify target node for job execution etc)

Note: 0 in executor specify no job will run on master. we can specify label to run jobs on slave always. 
global configuration (executors) at the system level sets a default number of executors for the Jenkins instance, while the node-specific configuration allows you to customize the number of executors for each individual node. 

Each executor represents a slot for running a job, so increasing the number of executors allows more jobs to run concurrently on that specific node.

1. Number of Executors at Node Level:
Increasing the number of executors at the node level allows for concurrent builds on that node. If the executor count is set to 1, only one build can run at a time on the node. If the executor count is greater than 1, multiple builds can run concurrently on that node.

2. "Execute concurrent builds if necessary" Option:
Enabling this option allows Jenkins to queue up additional builds for a job, labeled "linux" in your example, even if the job is currently executing on a node. Builds will be queued until an executor becomes available. If there are three executors and you run the job four times, Jenkins will execute three builds concurrently, and the fourth build will be queued.

3. Throttle Builds Option:
Throttle Builds helps limit the number of concurrent builds for a specific job or group of jobs. If you have three executors and set the throttle build count to 2 for a job labeled "linux," only two builds of that job will execute concurrently, and any additional builds will be queued until an executor becomes available. If you run the job four times, only two builds will execute in parallel, and the remaining two will be queued.

Security:
Athentication: The process of verifying the user's identity. who the user is.
Authorization: The process of determining what actions the verified user is allowed to perform.

1. credentials: 
	1. username and password
	2. certificate
	3. ssh username with privateKeys
	4. secret text (like tokens)
	5. secret file

2. Configure global security	--> authorization --> matrix based security 
Enabling Security: Enable security to ensure a secure Jenkins environment.
Access control: 
	1.Security realm (Authentication): 
		Jenkins own user database (allow to signup), LDAP, Unix user/group database.
	2. Authorization: 
		Checkbox options:
		1. "Anyone can do anything" (less secure).
		2. Legacy mode.
		3. Logged-in users can do anything.
		4. Anonymous read access (viewing jobs without manipulation).
		5. Matrix-based security (granular permissions).
		6. Project-Based Matrix Authorization Strategy

Matrix based security:  In the matrix, you can define permissions for specific users or groups for each job or project(contexts). 
Inheritance authorization Strategies: 
	1. Inherit from Parent: Permissions cascade from parent to child (for projects in a folder).
	2. Inherit Globally Defined Permissions: Only global permissions are applied (ignores folder permissions).
	3. Do Not Inherit: No permissions inherited from ancestors.
Note: Jenkins follows an explicit allow model.

Project-Based Matrix Authorization Strategy: Granular permissions at the project level. Explicitly setting permissions at specific job or folder levels.

Role-Based Access Control (RBAC): Global security --> enable role based strategy --> manage roles option will appear under security
	1. Jenkins supports RBAC, allowing you to create and assign roles with specific permissions.
	2. Install and configure the "Role-based Authorization Strategy" plugin through the Jenkins Plugin Manager.
	3. Define roles and assign them to users or groups, specifying permissions for each role.
	4. Roles can be job-specific, allowing you to control access at the project level.

3. manage users
4. configure credential provider: A location configured for Jenkins to retrieve credentials. ex: LDAP, Active Directory, SSO,any configured source.

status information


-----------------Freestyle Jobs----------------
default job types in jenkins. they are highly configurable through UI and allow you define ad-hoc tasks(scp,docker run),running shell commands, invoke ansible playbook clone git etc

General:
1. description
2. Discard old builds (days + number of builds to keep)
3. GitHub project
4. This project is parameterized
5. Throttle builds
6. Execute concurrent builds if necessary	
7. Restrict where this project can be run (labels)
8. Advanced: Quite period,retry count,Block build if upstream/downstream project is building,use custome workspace [checkboxes]

SCM:
1. None
2. Git

Build Triggers:
1. Trigger builds remotely (from script)
2. Build after other projects are build (upstream and downstream jobs)
3. Build periodically  (cronjob)
4. GitHub hook trigger for GitSCM polling
5. Poll SCM (cronjob) but build only if any SCM changes. 

Note: 3 and 5 both are cron jobs however 3rd run the job on mentioned time irrespective of any change in gitHub repo. 5th will try to run the job on mentioned time only if any change in gitHub repo otherwise skip.  

Build Environment:
1. Delete workspace before build starts
2. Use secret text or fields			--> env variable with all types of credentials(mentioned in global credentials menu). use this env var in shell
3. Add timestamp  to the console output
4. Inspect build log for published build scans
5. Terminate build if its stuck (gives timeout time)
6. with Ant

Build steps:
1. Execute shell/windows
2. Invoke Ant/Gradle script
3. Invoke top level maven 		--> if Maven integration plugin
4. Run with timeout

Post Build actions:


-------------------Jenkins configuration as code JCasc-----------------------
Jenkins as code refers to the practice of defining and configuring jenkins using code. Instead of manually configuring jenkins through user interface, Jcasc allows you to specify its configuration using prg language or declarative file. 

1. say we have installed jenkins
2. we have to install plugins so we will put plugins list in .txt file
3. write bash script to iterate through file and install plugins using jenkins-cli
for plugin in $(cat plugins.txt) 
do
 java -jar jenkins-cli.jar -s http://your-jenkins-url -auth admin:password install-plugin $plugin
done
java -jar jenkins-cli.jar -s http://your-jenkins-url -auth admin:password safe-restart

---configure jenkins----
1. Create a yaml file which includes configuration for:
  1. env variable 
  2. Credentials
  3. Tool configuration 
  4. Freestyle and pipeline jobs
save above file at location which is accessible to jenkins (/var/lib/jenkins)
2. Manage-jenkins --> Configuration as code (plugin.txt should have configuration as code plugin) --> put yaml file location ---> click on "Apply new configuration" 
 (reload existing configuraton, view configuration, Download configuration) 

----------------------------------------------------------------------------------------

---Git Integration with Jenkins-----
1. download Git in Jenkins vm if Jenkins running as service
2. provide GIT path as PATH env variable under global properties section of configure system section
3. use shell under build steps (git clone $USER:$PASS@url)		--> $USER and $PASS env var defined under build environment --> secret text or fields

or 
1. manage jenkins --> manage plugin --> download GIT without restart
2. provide Git executable path in Global tool configuration under GIT section if GIT already exist on jekins machine or auto download
3. check SCM  --> Git option by providing repo url and branch 


----Maven integratin with Jenkins----
1. download maven in jenkins machine
2. provide Maven path as PATH env variable under global properties section of configure system section
3. use shell under build step to build maven projects by specifying maven goals

or
1. manage jenkins -> manage plugins --> download maven integration plugin
2. manage jenkins --> global tool configuration --> maven configuration section --> provide maven variable name and path if maven is already available in jenkins machine otherwise select auto download with correct maven version 
3. select invoke top level maven under build steps by specifying goals




----Trigger jobs remotely through script,curl,postman----
ask for authentication token	--> provide any value like test123
JENKINS_URL/jobs/jobname/build?Token=TOKEN_NAME(above value)	or /buildWithParameters?Token=TOKEN_NAME		--> use to remotely trigger job


1. Through curl
Trigger jobs through curl requires authentication with jenkins with username and token so manage Jenkins --> security --> manage users --> click on user(say admin) --> configure --> API token --> create one
curl --user admin(user):token JENKINS_URL/jobs/jobname/build?Token=test123
curl -X POST -d 			--> if build with parameter

2. postman
put above endpoint in GET and provide username and token under authrization 

3. script --> in postman --> code snippet --> select any language to create script 
config.json
{
"username":"admin"
"token":"tokenvalue"
"jenkins_url":"JENKINS_URL"
"job_name":"RemoteTrigger"
"job_token":"test12"
}


main.py
#import libraries
import json
import os
import requests

#read config file
dir_path = os.path.dirname(os.path.realpath(__file__))			--> returns path of config.json file
configFile = open(dir_path + "/config.json","r")
configContent = configFile.read()					--> returns string
jsonConfigContent = json.loads(configContent)				--> return json as key-value pair (type will be dict)

#parse config file
jenkins_url = jsonConfigContent('jenkins_url')
user = jsonConfigContent('username')
token = jsonConfigContent('token')
job_name = jsonConfigContent('job_name')
job_token = jsonConfigContent('job_token')

#trigger job
URL = jenkins_url+'/jobs/'+job_name+'/build?token='+job_token
response = requests.post(URL, auth=(user,token))

#check status code
if response.status_code == 201:
   print("SUCCESS")
else:
   print("FAILURE")


------------------------------------Jenkins GitHub webhook interation-----------
Webhooks are one of the ways web application can communicate with each other. it allows you to send real time data from one apps to another whenever a given event occurs. 

webhook vs API
With API you get data through a process known as polling.  this is when your application periodically makes a request to an API server to check for new data. ex. weather apps
Webhook, pushes the data to receiving application as soon as an event occur. thats why called as reverse API. 

GitHub repo --> settings --> webhook --> add webhook --> JENKINS_URL/github-webhook/			--> in recent deliveries you can see github shares payload with jenkins 


-----Generic webhook trigger to parse webhook payload----------
we can use the generic webhook trigger plugin to parse webhook payload to trigger jenkins jobs based on particular action like PR opened or closed etc in github by filtering the webhook payloads in jenkins

jobs --> job_name --> configure -> generic-webhook --> provide token value --> post content parameters (variable like ACTION,expression like $.action)--> optional filter (expression like opened, text like $ACTION) --> build steps

repo --> settings --> webhook --> add webhook --> JENKINS_URL/generic-webhook-trigger/invoke?Token=TOKEN_VALUE 


------------------------------------Folder properties plugin---------------------------
Allows user to define properties for folder which can then be used by any jobs contained within it or any of its sub-folder. 


---------------------------------Build with parameters--------------------------
Build parameters allows you to pass data into jenkins job. Supported parameters include Strings, Boolean, choices, Files,Credentials, password, URL to another job etc. 
In FreeStyle job,you can access parameter just like any env variable ${PARAMETER_NAME}

curl --user admin(user):token "JENKINS_URL/jobs/jobname/buildWithParameters?Token=test123&IMAGE_NAME=test&IMAGE_TAG=1.11&ENVIRONMENT=PROD"
curl -X POST --user admin(user):token "JENKINS_URL/jobs/jobname/buildWithParameters?Token=test123" --data "IMAGE_NAME=test&IMAGE_TAG=1.11&ENVIRONMENT=PROD"

---Active choices plugin---
used in parameterized freestyle jenkins job to create scripted,dynamic and interactive job parameters. Active choices parameters can be dynamically updated and can be 
rendered as combo-boxes, radio buttons or rich html UI widgets. 

active-reactive parameters:
State			cities
Maharashtra		Pune,Mumbai,Nagpur
UP			Waranasi,Lukhnow,NCR
MP			Indore,Bhopal


trigger buid with parameters job remotely:
config.json
{
"username":"admin"
"token":"tokenvalue"
"jenkins_url":"JENKINS_URL"
"job_name":"RemoteTrigger"
"job_token":"test12"
"isTheJobParameterized: "True"
"my_data":{
"DOCKERHUB_USERNAME":"Shantayya"
"TAG":"5.55"
"PUSH":"true"
"DEPLOY_TO":"PROD"
   }
}
main.py
#import libraries
import json
import requests
import requests.auth
import HTTPBasicAuth

#read json file
withOpen("config.json","r") as f:
 data = json.load(f)

#parse json file
jenkins_url = data["jenkins_url"]
user = data["username"]
token = data["token"]
job_name = data["job_name"]
job_token = data["job_token"]
parameterized = data["isTheJobParameterized"]

#call remote job
if parametrized:
  URL = jenkins_url+'/jobs/'+job_name+'/buildWithParameters?token='+job_token
  my_data = data["my_data"]
else:
  URL = jenkins_url+'/jobs/'+job_name+'/build?token='+job_token
  my_data = None

result = requests.post(URL,auth=HTTPBasicAuth(user,token),data = my_data)

#status code
if int(result.status_code) == 201:
 print("Job Executed")
else:
 print("Job Failed")



-------------------------Jenkins Tomcat integration--------------
tomcat installation:
cd /opt
curl -O https://URL/apache-tomcat-9.0.56.tar.gz
tar -xzvpf apache-tomcat-9.0.56.tar.gz
mv apache-tomcat-9.0.56 tomcat9
echo "export CATALINA_HOME="/opt/tomcat9" >>~/.bashrc
source ~/.bashrc


start---> cd /opt/tomcat9/bin	--> ./startup.sh
access --> Access tomcat from your browser at http://<IP-Address>:8080  change port at conf/server.xml

add users --> add users and role at conf/tomcat-users.xml
enable remote access --> at webapps/manager/META-INF/context.xml

Manage jenkins --> manage plugins --> depoy to container plugin 
add tomcat credentials in manage jenkins --> security --> credentials  --> Global --> add credentials 

freestyle job --> post build actions --> deploy war to container --> 
WAR/EAR Files: 
context path: 
containers --> credentials + Tomcat URL


--------------------MailHog SMTP Jenkins integration-----------------
MailHog is an email testing tool  for developers. It allows to configure your application  to use MailHog for SMTP delivery.
MailHog runs with SMTP port 1025 and web interface on 8025. 

docker run -d -p 1025:1025 -p 8025:8025 --name mail mailhog/mailhog
manag jenkins --> configure system --> system configuration --> email notification --> add SMTP server and port details with recepient,default content etc

jobs --> configure --> post build tasks --> select email notification 

 
--------------------------Update Jenkins build status badge on GitHub-----------------
Manage Jenkins --> manage plugins --> embeddable build status plugin 
Goto FreeStyle job with SCM --> click on Embeddable build status --> copy one of markdown url in Git readme.md file 


--------------------Update Jenkins build status in GitHub Pull Request----------------
Generic-webhook trigger plugin needed to parse webhook payload to trigger jenkins jobs based on particular action like PR opened or closed etc in github by filtering the webhook payloads in jenkins
Post Build task plugin --> allowed to execute shell task depending on build log output. 

1. add webhook in github for PR open
jobs --> job_name --> configure -> generic-webhook --> provide token value --> post content parameters (variable like ACTION,expression like $.action)--> optional filter (expression like opened, text like $ACTION) --> build steps

repo --> settings --> webhook --> add webhook --> JENKINS_URL/generic-webhook-trigger/invoke?Token=TOKEN_VALUE 

2. send Jenkins build status to Github
Build actions --> post build task --> Log text as SUCCESS/FAILURE, script as below command  

curl -u USERNAME:USER_TOKEN(GitHub) -X POST "https://api.github.com/repos/GITHUB_USERNAME/REPO_NAME/statuses/COMMIT_ID" -H
"Accept: application /vnd.github.v3+json" -d 
"{\"state\": \"SUCCESS/FAILURE\", 
\"context\": \"ANY MESSEGE\",
\"description\":\"Jenkins\",
\"target_url\":\"JENKINS_URL/job/$JOB_NAME/$BUILD_NUMBER/console\" }"

curl -u shantayya:ghp_www -X POST "https://api.github.com/repos/shantayya/maven-webapp/statuses/$GIT_COMMIT" -H "Accept: application/vnd.github.v3+json"
-d "{
\"state\":\"success\",
\"context\":\"Continuous Integration\",
\"description\":\"Jenkins\",
\"target_url\":\"JENKINS_URL/job/$JOB_NAME/$BUILD_NUMBER/console\"
}"

refer: 
https://docs.github.com/en/rest/using-the-rest-api/getting-started-with-the-rest-api?apiVersion=2022-11-28
https://docs.github.com/en/rest/commits/statuses?apiVersion=2022-11-28#create-a-commit-status



-------------------Nexus Jenkins integration------------------------
manage jenkins --> manage plugins --> download nexus platform plugin
manage jenkins --> configure system --> system --> add Sonartype Nexus server details and save
create freestyle job --> configure --> build steps --> Nexus repository manager publisher --> select details from drop down(it will list repos availabe on nexus)

Note: under TAG, Snapshot version artifact will not be send to nexus repo from nexus 3 onwards. 


---------------------SonarQube Integration with Jenkins--------------
1. manage jenkins --> manage plugins --> downlaod sonarqube scanner plugin 
2. manage jenkins ---> configure system --> system configuration --> add Sonarqube Server details and add sonarqube server credentials under global credentials security section of managed jenkins 
3. manage jenkins --> global tool configuration --> provide sonar scanner path if scanner already exist in jenkins machine otherwise auto download
4. create freestyle job ---> configure --> build steps --> select execute sonarqube scanner --> provide sonar-project.properties file or provide all properties in text box
 


-------------------------Jenkins pipeline-----------------------------
Pipeline is series of events or tasks which are interconnected in particular order. in simple words, its combination of plugins that support integration and implementation of continuous delivery pipelines. 
DSL (domain specific lang) specialized to particular apps domain like HTML for web pages etc

definition of jenkins pipeline is written into text file called as Jenkinsfile. can be constructed in 2 ways,
1. Declarative Pipeline		--> very easy to understand and write 
pipeline{ 
 agent any
 stages {
   stage('Build'){steps{}}
   stage('Deploy'){ steps{script{}} }			--> script block helps to run groovy code inside declarative pipeline. 
  }
}
2. Scripted Pipeline (Groovy)	--> very powerful however need skilled resource and makes pipeline complicated
node{
stage('Build'){ steps {} }
stage('deploy'){steps {} }
 }

node/agent is crutial as it allocates an executor and workspace for pipeline. 

Node: Use Groovy sandbox if checked, runs the script in sandbox with limited abilities. if unchecked and you are not admin then you need to wait for admin approval for script execution. 
PATH=$PATH:$M2_HOME/bin
/usr/bin:/usr/sbin/:/opt/maven/bin		--> in Pipeline PATH=$PATH will not resolve so we use PATH+EXTRA in env variable or use withEnvironment


----shell commands inside pipeline-----
pipeline{
 agent any/None/Label/Docker
 stages{
   stage('Build'{
     steps{
        sh 'mvn clean'			--> When you use the sh step in a Jenkins pipeline, it's a built-in step (plugin) that allows you to execute shell commands
	sh '''				--> execute multiple commands 
		----------
		--------
 	   '''
           }
         }
     }
 }

-------------Timeout and retries------
We can retry certain steps in pipeline if they are failing or exit if steps take too long. (timeout)
when step cannot be completed then timeout help the controller avoid wasting resources and release executors. 
syntax: options {timeout(time: 1,unit: 'HOURS')}
	options {retry(3)}

pipeline{
  agent any
  options {
     timeout(time:1, unit:'HOURS')}
     retry(3)					--> global entry
    }
  stages{
    stage('Deploy'){
      steps{
              retry(3){							--> stage level options
                          sh './flaky.sh'
                      }
            timeout(time:3, unit: 'MINUTES'){
             sh './healthcheck.sh'
          }
       }
    }
}


----Post build steps------
post{
  always{
    echo 'This will run always' 
        }
  success{
     echo 'This will run only if job success'
         }
  failure{
     echo 'This will run only if job fails'
         }
  unstable{
     echo 'This will ony run if job turns to unstable'
         }
  changed{
      echo 'This will run only if job status has changed. be it from success to failure and vice versa'
         }
    }


-------------Environment variable--------
pipeline{
  agent any
  tools{
    maven 'M3'						--> use maven tool if maven path is set in gloabal tool conf so no need to witEnv() wrapper to specify mvn path 
      }
  environment{						--> use globally
   USE_JDK='true'
   JDK_HOME='/opt/jdk1.8.0_333'
   DB_ENGINE='sqlite'
   GIT_CREDS = credentials('GIT_USR_PASS')		--> GIT_USR_PASS credential mentioned in security--> credentials section
      }
   stages{
     stage('Print Env variables'){	
             environment{ ----}				---> stage level
             steps{
                   echo '${DB_ENGINE}' or $DB_ENGINE or ${env.DB_ENGINE}
		   echo '${GIT_CREDS_USR}'	
	           echo '${GIT_CREDS_PASS}'
                  }
           }
         }
     stage('Build'){
       staps{
          withEnv(['PATH+EXTRA=/opt/maven/bin']){	--> use withEnv() wrapper if tools path is not mentioned in global tool configuration. generate withEnv using pipeline syntax generator
              sh 'mvn clean install'
             }
          }
       }
 }


Note: if var contains username and password then we can use var_USR and var_PASS to separate username and password in jenkins. 
if you are using env variable under script{} then always use env.VAR_NAME to access env variable otherwise it will not work

1. credentials()					--> expose sensitive info in logs plus globa scope if defined in global environment block
GIT_CREDS = credentials('GIT_USR_PASS')
curl -u GIT_CREDS_USR:GIT_CREDS_PASS 'URL'

2. using withCredentials()				--> not expose plus limited scope
withCredentials([usernamePassword(crdentialsId:'git_usr_pass', passwordVariable:'Y', usernameVariable:'X')])
 {
   curl -u $X:$Y 'URL'
 }

while both methods achieve the same purpose, using withCredentials() is often considered a best practice for handling credentials in Jenkins pipelines.

----parallel block---------------
A stage must have one and only one of steps, stages, parallel, or matrix. It is not possible to nest a parallel or matrix block within a stage directive if that stage directive is nested within a parallel or matrix block itself. However, a stage directive within a parallel or matrix block can use all other functionality of a stage, including agent, tools, when, etc.

Note: you can force your parallel stages to all be aborted when any one of them fails, by adding failFast true to the stage containing the parallel. Another option for adding failfast is adding an option to the pipeline definition: parallelsAlwaysFailFast().

example: 
pipeline {
    agent any
    options{
       parallelAlwaysFailFast()
    }
    stages {
        stage('Non-Parallel Stage') {
            steps {
                echo 'This stage will be executed first.'
            }
        }
        stage('Parallel Stage') {
            when {
                branch 'master'
            }
            failFast true						or 
            parallel {
                stage('Branch A') {
                    agent {
                        label "for-branch-a"
                    }
                    steps {
                        echo "On Branch A"
                    }
                }
                stage('Branch B') {
                    agent {
                        label "for-branch-b"
                    }
                    steps {
                        echo "On Branch B"
                    }
                }
                stage('Branch C') {
                    agent {
                        label "for-branch-c"
                    }
                    stages {
                        stage('Nested 1') {
                            steps {
                                echo "In stage Nested 1 within Branch C"
                            }
                        }
                        stage('Nested 2') {
                            steps {
                                echo "In stage Nested 2 within Branch C"
                            }
                        }
                    }
                }
            }
        }
    }
}

---------matrix feature------------
The Jenkins matrix feature allows you to execute a set of build steps in multiple configurations, often referred to as "axes". It's particularly useful when you want to test your software across different environments, platforms, or configurations.

The matrix section must include an axes section and a stages section. The axes section defines the values for each axis in the matrix. The stages section defines a list of stages to run sequentially in each cell.

axes:
The axes section specifies one or more axis directives. Each axis consists of a name and a list of values. All the values from each axis are combined with the others to produce the cells.

stages:
The stages section specifies one or more stages to be executed sequentially in each cell.

Example- One-axis with 3 cells, each cell runs three stages - "build", "test", and "deploy"
matrix {
    axes {
        axis {
            name 'PLATFORM'
            values 'linux', 'mac', 'windows'
        }
    }
    stages {
        stage('build') {
            // ...
        }
        stage('test') {
            // ...
        }
        stage('deploy') {
            // ...
        }
    }
}

Example: Complete martrix example
pipeline {
    parameters {
        choice(name: 'PLATFORM_FILTER', choices: ['all', 'linux', 'windows', 'mac'], description: 'Run on specific platform')
    }
    agent none
    stages {
        stage('BuildAndTest') {
            matrix {
                agent {
                    label "${PLATFORM}-agent"
                }
                when { anyOf {
                    expression { params.PLATFORM_FILTER == 'all' }
                    expression { params.PLATFORM_FILTER == env.PLATFORM }
                } }
                axes {
                    axis {
                        name 'PLATFORM'
                        values 'linux', 'windows', 'mac'
                    }
                    axis {
                        name 'BROWSER'
                        values 'firefox', 'chrome', 'safari', 'edge'
                    }
                }
                excludes {
                    exclude {
                        axis {
                            name 'PLATFORM'
                            values 'linux'
                        }
                        axis {
                            name 'BROWSER'
                            values 'safari'
                        }
                    }
                    exclude {
                        axis {
                            name 'PLATFORM'
                            notValues 'windows'
                        }
                        axis {
                            name 'BROWSER'
                            values 'edge'
                        }
                    }
                }
                stages {
                    stage('Build') {
                        steps {
                            echo "Do Build for ${PLATFORM} - ${BROWSER}"
                        }
                    }
                    stage('Test') {
                        steps {
                            echo "Do Test for ${PLATFORM} - ${BROWSER}"
                        }
                    }
                }
            }
        }
    }
}


------Deploying on pods------
1. install kubectl on jenkins(running as service) machine  and put kubectl executable in /usr/bin and provide executable permission for kubectl
2. declare PATH variable in manage jenkins --> configure systems --> system configurations --> environment properties --> PATH
3. downlaod kubeconfig file from digital ocean and create secret file from manage jenkins --> security --> credentials --> dc_kc
4. create pipeline job
pipeline{
  agent any								--> When you use agent any, Jenkins will dynamically allocate an available agent (which, in 									this case, could be a Kubernetes pod) to execute the pipeline stages. This flexibility allows 									Jenkins to choose any available agent that meets the requirements of the pipeline stages.
  environment{
   my_config = credentials('dc_kc')
      }
  stages{
    stage('Get Pods'){
      steps{
              sh 'kubectl --kubeconfig=$my_config get pods'
            }
        }
    }
}

Note: here we have to provide --kubeconfig everytime we execute kubectl command 

--using kubenetes CLI plugin---
1. install kubectl on jenkins machine  and put kubectl executable in /usr/bin and provide executable permission for kubectl
2. manage jenkins--> manage plugins --> kubenetes cli
3. create secret file from manage jenkins --> security --> credentials --> dc_kc (file having kubeconfig file)
4. create pipeline job
pipeline{
  agent any
  stages{
    stage('Get Pods'){
      steps{
	  script{
		withKubeConfig([credentialsId='dc_kc']){		--> withKubeConfig() wrapper is scripted syntax so used in script block
   	           sh 'kubectl get pods'
        	    }
		}
        }
    }
  }
}


Note: if we have to use helm then install it on jenkins machine and follow remaining steps from 2 to 4.
kubeconfig file handles the authentication and access to the Kubernetes cluster, and no additional service account or role binding is needed within the Kubernetes cluster for Jenkins in this specific pipeline configuration. 



--------Push maven artifacts to Nexus repository---------------
We need Sonatype Nexus Artifact Uploader(for uploading artifact) and Pipeline Utility Steps (for fetching artifact detaits from pom) plugins to download in jenkins. 
referrence: https://blog.sonatype.com/workflow-automation-publishing-artifacts-to-sonatype-nexus-using-jenkins-pipelines

pipeline{
  agent any
  tools{
  maven 'M3'
      }
  environment{
     NEXUS_VERSION = "nexus3"
     NEXUS_PROTOCOL = 'http"
     NEXUS_URL="http://ip:8081"
     NEXUS_REPOSITORY="maven-nexus-repo"
     NEXUS_CREDENTIAL_ID = "nexus credentials"
    }
   stages{
       stage('Clone Repo"{
              steps{
                   git branch: 'master',
                   url: 'url"
                   }
                }
       stage('Build'){
             steps{
                sh 'mvn -DskipTests/-Dmaven.test.failure.ignore=true clean package'
                  }
               }
        stage('Push artifact to Nexus'){
             steps{
                script{
                    pom = readMavenPom file: "pom.xml";
                    filesByGlob = findfiles(glob: "target/*.${pom.packaging}");
                    echo "${filesByGlob[0].name} ${filesByGlob[0].path} ${filesByGlob[0].directory} ${filesByGlob[0].length} ${filesByGlob[0].lastModified}"
                    artifactPath = filesByGlob[0].path;
                    artifactExists = fileExists artifactPath;
                    if(artifactExists){
                         echo "****File: ${artifactPath}, group: ${pom.groupId}, packaging: ${pom.packaging}, version: ${pom.version}";
                         nexusArtifactUploader(
                              nexusVersion: NEXUS_VERSION,
			      nexusProtocol: NEXUS_PROTOCOL,
                              nexusUrl: NEXUS_URL,
			      groupId: pom.groupId,
                              version: pom.version,
 			      repository: NEXUS_REPOSITORY,
                              credentialId: NEXUS_CREDENTIAL_ID,
                              artifacts: [
                                   [artifactId: pom.artifactId,
                                    classifier: '',
                                    file: artifactPath,
                                    type: pom.packaging],
                                    
                                    [artifactId: pom.artifactId,
                                    classifier: '',
                                    file: pom.xml,
                                    type: pom]

					]
				);
			}else{
                                 error "*** File: ${artifactPath}, could not be found";
                         }
                   }
	   }
    }


Nexus push via CURL
referrence : 
https://support.sonatype.com/hc/en-us/articles/115006744008-How-can-I-programmatically-upload-files-into-Nexus-3-

withCredentials([usernamePassword(credentialsId: 'CREDS', passwordVariable: 'PASS', usernameVariable: 'USER')]) {
    sh "curl -v -u $USER:$PASS --upload-file target/${pom.artifactId}-${pom.version}.${pom.packaging} \
    http://NEXUS-SERVER-URL/repository/maven-hosted/org/springframework/samples/${pom.artifactId}/${pom.version}/${pom.artifactId}-${pom.version}.${pom.packaging}"
}


--------------Using K8s pods as Jenkins build agents-------------------------
If you are not using a kubeconfig file then need to set up a service account, role, and role binding for Jenkins within your Kubernetes cluster so that Jenkins which is running as service out of K8s cluster will talk to K8s cluster. 

#Jenkins-sa.yaml:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: Jenkins-sa

#Jenkins-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: Jenkins-role
rules:
  apiGroups: [""]
  resources: ["pods","pods/exec,"pods/log"]
  verbs: ["get","list","watch","create","patch","update","delete"]

#Jenkins-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: Jenkins-role-binding
subjects:
- kind: ServiceAccount
  name: Jenkins-sa
rolRef:
  kind: Role
  name: Jenkins-role
  apiGroup: rbac.authorization.k8s.io

kubectl apply -f Jenkins-sa.yaml
kubectl apply -f Jenkins-role.yaml
kubectl apply -f Jenkins-role-binding.yaml

pipeline{
  agent{
    kubernetes{
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
       '''}
     }
    stages{
        stage('Checkout SCM'){
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
              }
         }
  }


Note: if you want to run Jenkins as container then use helm chart to install jenkins. Jenkins if running as pod then it always spins JNLP container within all slave pods for communication and JNLP container runs on port 50000. when we install jenkins using helm chart, JNLP container template configuration are in built. 


-----------------------inheritFrom concept-------------------------------
In Jenkins pipelines, when using Kubernetes pods as agents, the "inheritFrom 'default pod template name'" statement is used to inherit configuration settings from a predefined pod template defined in jenkins. This is a way to reuse a set of common configurations for multiple agents.

Here's how it typically works:
1. Jenkins Master Setup:
Jenkins, running in a container or another deployment in your Kubernetes cluster, has access to DockerHub.
Jenkins is configured with the default pod template, which includes the JNLP container and any other necessary configurations.

2. Agent Pod Creation:
When a Jenkins job is triggered and requires an agent, Jenkins dynamically creates a new pod based on the configured pod template.

3. Docker Image Pull:
If the pod template specifies a Docker image from DockerHub (e.g., the JNLP container or any other tools), Jenkins will attempt to pull that image.

4. Jenkins as Docker Registry Proxy:
Jenkins acts as a proxy for DockerHub in this case. It pulls the necessary images from DockerHub and caches them locally.
The Jenkins master then serves as the registry for the Jenkins agents within the Kubernetes cluster.

5. Agent Pod Execution:
Once the pod is created and the necessary Docker images are available locally in the Jenkins master, the agent pod is launched with the specified containers.

// Shared pod template named 'default'
podTemplate(name: 'default', label: 'mypod', containers: [
    containerTemplate(name: 'maven', image: 'maven:alpine', command: 'cat', ttyEnabled: true),
    containerTemplate(name: 'git', image: 'git:alpine', command: 'cat', ttyEnabled: true)
])

pipeline {
    agent {
        kubernetes {
            inheritFrom 'default' // Inherit configurations from the 'default' pod template

            // Local override: Specify additional containers or modifications
            containers {
                containerTemplate(name: 'customContainer', image: 'custom/image', command: 'cat', ttyEnabled: true)
            }
        }
    }

    stages {
        stage('Checkout SCM') {
            steps {
                container('git') {
                    // Local step specific to the 'git' container
                    sh 'git clone URL'
                }
            }
        }
        stage('Build') {
            steps {
                container('maven') {
                    // Local step specific to the 'maven' container
                    sh 'mvn clean -DskipTests package'
                }
            }
        }
    }
}

If you don't provide any additional pod manifest in the kubernetes block and use inheritFrom 'default', Jenkins will use the inherited configurations as the pod template for the agent. In this scenario, the 'default' pod template would define the containers, images, volumes, and other settings for the agent pod.


---------------------------------------------------------------End to End pipeline----------------------------------------------------------------------
Pipeline includes pods as build agents,maven,nexus,sonarqube and helm
#To see the containers running inside pods:
kubectl get pods <pod-name> -o jsonpath='{.spec.containers[*].name}'
kubectl exec -it <pod-name> -c <container-name> -- bash					--> to get into container

1. create k8s cluster on digital ocean and downlaod kubeconfig file locally
2. download helm and kubectl and set path 
3. install jenkins through helm chart
helm repo add jenkins https://charts.jenkins.io
helm repo update
helm pull jenkins/jenkins --untar			

helm upgrade --install jenkins jenkins/ 					--> you have to make below changes in values.yaml file and then execute
or 
helm upgrade --install jenkins jenkins/jenkins --set controller.servicePort=80 --set controller.serviceType=LoadBalancer		--> changing service type to loadbalancer so that jenkins can be directly accessible from outside cluster

kubectl exec -it jenkins-0 -- bash
cat /run/secrets/additional/chart-admin-password

pipeline{
  agent{
    kubernetes{
      //inheritFrom 'default'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
       '''}
     }
    stages{
        stage('Checkout SCM'){
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
              }
         }
  }


Note: if you want to run Jenkins as container then use helm chart to install jenkins. Jenkins if running as pod then it always spins JNLP container within all slave pods for communication and JNLP conrainer accepts task from jenkins on port 50000. when we install jenkins using helm chart, JNLP container (inbound agent) template configuration are in built. 

When using Jenkins with Kubernetes agents, the common approach for sharing files between containers in a pod is to use shared volumes. The emptyDir volume is one option. Jenkins uses the Kubernetes Plugin, which supports several ways of sharing data between containers in a pod. When you define a pod template in Jenkins using the Kubernetes Plugin, you can specify volumes that should be mounted into the containers.


----------------JUnit plugin + PVC for maven------------------
download JUnit plugin for showing unit test results and use PVC for maven so that maven will not download dependancies every time which will delay the job execution

#pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: maven-cache
spec:
  accessModes:
  - readWriteOnce
  resources:
    requests:
      storage: 1Gi

pipeline{
  agent{
    kubernetes{
      //inheritFrom 'default'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
	  volumeMounts:
          - name: maven-cache
            mountPath: "/root/.m2/repository/"
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
        volumes:
        - name: cache
          persistentVolumeClaim:
              claimName: maven-cache
 
       '''}
     }
    stages{
        stage('Checkout SCM'){
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
		post{
		   success{
                     JUnit '**/target/reports/*.xml'
 			}
                     }
              }
         }
  }



--------sonar scan------
1. install sonarqube using helm charts
helm repo add sonarqube https://SonarSource.github.io/helm-chart-sonarqube
helm repo update
helm pull sonarqube/sonarqube --untar			--> update values.yaml and change service type as loadbalancer

helm upgrade --install sonarqube sonarqube/ 			--> password: admin
2. manage jenkins --> manage plugins --> download sonar scanner cli plugin 
3. manage jenkins ---> configure system --> system configuration --> add Sonarqube Server details and add sonarqube server credentials under global credentials security section of managed jenkins 
4. spin pod with sonar scanner cli 

pipeline{
  agent{
    kubernetes{
      //inheritFrom 'default'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
	  volumeMounts:
          - name: maven-cache
            mountPath: /root/.m2/repository/
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
        volumes:
        - name: cache
          persistentVolumeClaim:
              claimName: maven-cache
        - name: sonar-cli
          image: sonarsource/sonar-scanner-cli:latest
          command:
          - cat
          tty: true
       '''}
     }
    stages{
        stage('Checkout SCM'){
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
		post{
		   success{
                     JUnit '**/target/reports/*.xml'
			}
                     }
              }
         }
        stage('Sonar scan'){
           steps{
                container('sonar-cli'){
                 withSonarQubeEnv(credentialsId: 'sonar', installationName: 'sonarserver'){	--> withSonarQubeEnv() wrapper used to send scan details with server
                     sh ''' /opt/sonar-scanner/bin/sonar-scanner \  			--> execute sonar-scanner binary
			-Dsonar.projectKey=petclinic \
			-Dsonar.projectName=petcliic \
			-Dsonar.projectVersion=1.0 \
			-Dsonar.sources=src/main \
			-Dsonar.tests=src/test \
			-Dsonar.java.binaries=target/classes \
			-Dsonar.language=java \
			-Dsonar.sourceEncoding=UTF8 \
			-Dsonar.java.libraries=target/classes 
                       '''
                     }
                   }
                }
             }
          stage('Wait for quality gate'){
              steps{
                      container('sonar-cli'){
                         timeout(time: 1, unit: 'HOURS'){			--> create sonarqube-webhook in sonarqube so that it will send QG info to jenkins
                            waitForQualityGate abortPipeline: true		--> wait for quality get information to reach jenkins if not then abort pipeline
                            }
                          }
                    }
                }
          }


----------------send maven build to nexus------------------------
refer: https://artifacthub.io/packages/helm/sonatype/nexus-repository-manager

1. install nexus using helm chart
helm repo add sonatype https://sonatype.github.io/helm3-charts/
helm repo update
helm pull sonatype/nexus-repository-manager --untar

helm upgrade --install nexus nexus-repository-manager/
kubectl exec -it nexus-nexus-repository-manager-5745766765-5pjcf -- bash
cat /nexus-data/admin.password

2. download nexusArtifactUploader and pipeline utility steps plugin
3. manage jenkins --> global security --> credentials --> add nexus credentials as nexus-creds
4. spin pod with curl installed 

pipeline{
  agent{
    kubernetes{
      //inheritFrom 'default'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
	  volumeMounts:
          - name: maven-cache
            mountPath: /root/.m2/repository/
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
        volumes:
        - name: cache
          persistentVolumeClaim:
              claimName: maven-cache
        - name: sonar-cli
          image: sonarsource/sonar-scanner-cli:latest
          command:
          - cat
          tty: true
        - name: curl
          image: alpine/curl:latest
          command:
          - cat
          tty: true
       '''}
     }
     environment{
	NEXUS_PROTOCOL='http'
	NEXUS_VERSION='nexus3'
	NEXUS_URL='IP:8081'
	NEXUS_REPOSITORY='maven-hosted'
	NEXUS_CREDENTIAL_ID='nexus-creds'
         }

    stages{
        stage('Checkout SCM'){
	    when { expression { true } }
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
	     when { expression { true } }
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
		post{
                     JUnit '**/target/reports/*.xml'
                     }
              }
         }
        stage('Sonar scan'){
           when { expression { true } }
           steps{
                container('sonar-cli'){
                 withSonarQubeEnv(credentialsId: 'sonar', installationName: 'sonarserver'){	--> withSonarQubeEnv() wrapper used to send scan details with server
                     sh ''' /opt/sonar-scanner/bin/sonar-scanner \  			--> execute sonar-scanner binary
			-Dsonar.projectKey=petclinic \
			-Dsonar.projectName=petcliic \
			-Dsonar.projectVersion=1.0 \
			-Dsonar.sources=src/main \
			-Dsonar.tests=src/test \
			-Dsonar.java.binaries=target/classes \
			-Dsonar.language=java \
			-Dsonar.sourceEncoding=UTF8 \
			-Dsonar.java.libraries=target/classes 
                       '''
                     }
                   }
                }
             }
          stage('Wait for quality gate'){
              when { expression { true } }
              steps{
                      container('sonar-cli'){
                         timeout(time: 1, unit: 'HOURS'){			--> create sonarqube-webhook in sonarqube so that it will send QG info to jenkins
                            waitForQualityGate abortPipeline: true		--> wait for quality get information to reach jenkins if not then abort pipeline
                            }
                          }
                    }
                }
          stage('push maven to Nexus'){
	       when { expression { false } }
               steps{
                    container('jnlp'){
                          script{
				pom = readMavenPom file: "pom.xml";
				filesByGlob = findFiles(glob:"target/*.${pom.packaging}");
				artifactPath = filesByGlob[0].path;
				artifactExists = fileExists artifactPath;
				if (artifactExists){
					nexusArtifactUploader(
						nexusVersion: NEXUS_VERSION,
						protocol: NEXUS_PROTOCOL,
						nexusUrl: NEXUS_URL,
						groupId: pom.groupId,
						version: pom.version,
						repository: NEXUS_REPOSITORY,
						credential_Id: NEXUS_CREDENTIAL_ID,
						artifacts: [
						[artifactId: pom.artifactId,
						 classifiers: '',
						 file: artifactPath,
						 type: pom.packaging],
						
						[artifactId: pom.atifactId,
						 classifiers: '',
						 file: "pom.xml",
						 type: "pom"]
                                              ]
					);
						
				}else{
					echo "***FILE: ${artifactpath} could not be found";
				   }

                              }  
                          }
                       }
                  }
            stage('Push maven to nexus through CURL'){
	   	   when { expression { true } }
                   steps{
                        container('curl'){
                          script{
				withCredentials([usernamePassword(credentialsId:'nexus-creds', passwordVariable:'pass', userVariable:'usr')]){
					pom = readMavenPom file: "pom.xml";
					sh "curl -v -u $usr:$pass --upload-file target/${pom.artifactId}-${pom.version}-${pom.packaging} \
					http://$NEXUS_URL/repository/$NEXUS_REPOSITORY/org/springframework/samples/${pom.artifactId}/${pom.version}/							${pom.artifactId}-${pom.version}-${pom.packaging}"
						
					}
			        }
                             }
                        }
                    }

 		}
 	}
------------Build docker image from war file--------------------
1. spin docker cli container as k8s agent
2. add dockerhub credentials in manage jenkins --> global credentials --> credentials
3. mount /var/run/docker.sock hostPath in above container so that docker container (docker cli) can use of /var/run/docker.sock file to connect to docker daemon from host to perform docker commands. 
4. create dockerfile 
FROM openjdk:8-jre-alpine
COPY target/*.war /usr/bin/spring-petclinic.war
EXPOSE 8080
ENTRYPOINT ["java","-jar","/usr/bin/spring-petclinic.war","--server.port=8080"]


pipeline{
  agent{
    kubernetes{
      //inheritFrom 'default'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
	  volumeMounts:
          - name: maven-cache
            mountPath: /root/.m2/repository/
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
        - name: sonar-cli
          image: sonarsource/sonar-scanner-cli:latest
          command:
          - cat
          tty: true
        - name: curl
          image: alpine/curl:latest
          command:
          - cat
          tty: true
        - name: docker
          image: docker:latest
          command:
          - cat
          tty: true
          volumeMounts:
          - name: docker-sock
            mountPath: /var/run/docker.sock
        volumes:
        - name: cache
          persistentVolumeClaim:
              claimName: maven-cache
        - name: docker-sock
          hostpath: 
             path: /var/run/docker.sock
       '''}
     }
     environment{
	NEXUS_PROTOCOL='http'
	NEXUS_VERSION='nexus3'
	NEXUS_URL='IP:8081'
	NEXUS_REPOSITORY='maven-hosted'
	NEXUS_CREDENTIAL_ID='nexus-creds'
	DOCKERHUB_USER='shantayya'
	APP_NAME='petclinic'
	IMAGE_NAME="${DOCKERHUB_USER}"+ "/" + "${APP_NAME}"
	IMAGE_TAG=$BUILD_NUMBER
         }

    stages{
        stage('Checkout SCM'){
	    when { expression { true } }
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
	     when { expression { true } }
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
		post{
                     JUnit '**/target/reports/*.xml'
                     }
              }
         }
        stage('Sonar scan'){
           when { expression { true } }
           steps{
                container('sonar-cli'){
                 withSonarQubeEnv(credentialsId: 'sonar', installationName: 'sonarserver'){	--> withSonarQubeEnv() wrapper used to send scan details with server
                     sh ''' /opt/sonar-scanner/bin/sonar-scanner \  			--> execute sonar-scanner binary
			-Dsonar.projectKey=petclinic \
			-Dsonar.projectName=petcliic \
			-Dsonar.projectVersion=1.0 \
			-Dsonar.sources=src/main \
			-Dsonar.tests=src/test \
			-Dsonar.java.binaries=target/classes \
			-Dsonar.language=java \
			-Dsonar.sourceEncoding=UTF8 \
			-Dsonar.java.libraries=target/classes 
                       '''
                     }
                   }
                }
             }
          stage('Wait for quality gate'){
              when { expression { true } }
              steps{
                      container('sonar-cli'){
                         timeout(time: 1, unit: 'HOURS'){			--> create sonarqube-webhook in sonarqube so that it will send QG info to jenkins
                            waitForQualityGate abortPipeline: true		--> wait for quality get information to reach jenkins if not then abort pipeline
                            }
                          }
                    }
                }
          stage('push maven to Nexus'){
	       when { expression { false } }
               steps{
                    container('jnlp'){
                          script{
				pom = readMavenPom file: "pom.xml";
				filesByGlob = findFiles(glob:"target/*.${pom.packaging}");
				artifactPath = filesByGlob[0].path;
				artifactExists = fileExists artifactPath;
				if (artifactExists){
					nexusArtifactUploader(
						nexusVersion: NEXUS_VERSION,
						protocol: NEXUS_PROTOCOL,
						nexusUrl: NEXUS_URL,
						groupId: pom.groupId,
						version: pom.version,
						repository: NEXUS_REPOSITORY,
						credential_Id: NEXUS_CREDENTIAL_ID,
						artifacts: [
						[artifactId: pom.artifactId,
						 classifiers: '',
						 file: artifactPath,
						 type: pom.packaging],
						
						[artifactId: pom.atifactId,
						 classifiers: '',
						 file: "pom.xml",
						 type: "pom"]
                                              ]
					);
						
				}else{
					echo "***FILE: ${artifactpath} could not be found";
				   }

                              }  
                          }
                       }
                  }
            stage('Push maven to nexus through CURL'){
	   	   when { expression { true } }
                   steps{
                        container('curl'){
                          script{
				withCredentials([usernamePassword(credentialsId:'nexus-creds', passwordVariable:'pass', userVariable:'usr')]){
					pom = readMavenPom file: "pom.xml";
					sh "curl -v -u $usr:$pass --upload-file target/${pom.artifactId}-${pom.version}-${pom.packaging} \
					http://$NEXUS_URL/repository/$NEXUS_REPOSITORY/org/springframework/samples/${pom.artifactId}/${pom.version}/							${pom.artifactId}-${pom.version}-${pom.packaging}"
						
					}
			        }
                             }
                        }
                    }

	     stage('Docker build image'){
                   steps{
                        container('docker'){
					sh 'docker build -t $IMAGE_NAME:$IMAGE_TAG .'
					sh 'docker tag $IMAGE_NAME/$IMAGE_TAG $IMAGE_NAME:latest'
					withCredentials([usernamePassword(crdentialsId:'dockerhub', passwordVariable:'pass', userVariable:'user')]){
						sh 'docker login -u $user -p $pass'
						sh 'docker push $IMAGE_NAME/$IMAGE_TAG'
						sh 'docker push $IMAGE_NAME/latest'
                                         }
					sh 'docker rmi $IMAGE_NAME/$IMAGE_TAG'
					sh 'docker rmi $IMAGE_NAME/latest'
                                      }
                         }
                }
          }
   }


------------------Deploy to kubernetes cluster-----------------------
1. create dockerfile for creating image having helm and kubectl
FROM alpine/helm
RUN curl -LO https://dl.k8s.io/release/v1.25.0/bin/linux/amd64/kubectl \
    && mv kubectl /bin/kubectl \
    && chmod a+x /bin/kubectl

docker build -t helm-kubectl-cli .
docker push helm-kubectl-cli
 
2. download kubernetes cli plugin 
3. download kubeconfig file from digital ocean and manage jenkins --> manage credentials --> add secret file(kubeconfig file) as do-kc
4. download deployment.yaml file
apiVersion: v1
kind: Deployment
metadata:
  labels:
    app: petclinic
spec:
  replicas: 1
  selectors:
    matchLabels:
      app: petclinic
  template:
    metadata:
      labels:
        app: petclinic
    spec:
  	containers:
  	- name: petclinic
    	  image: shantayya/petclinc:latest
          ports:
          - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: petclinic
spec:
  type: Loadbalancer
  selector:
      app: petclinic
  ports:
  - port: 80
    containerPort: 8080
 
5. create  helm-kubectl container from step 1
pipeline{
  agent{
    kubernetes{
      //inheritFrom 'default'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: test
      spec:
        containers:
        - name: maven
          image: maven:alpine
          command:
          - cat
          tty: true
	  volumeMounts:
          - name: maven-cache
            mountPath: /root/.m2/repository/
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
        - name: sonar-cli
          image: sonarsource/sonar-scanner-cli:latest
          command:
          - cat
          tty: true
        - name: curl
          image: alpine/curl:latest
          command:
          - cat
          tty: true
        - name: docker
          image: docker:latest
          command:
          - cat
          tty: true
          volumeMounts:
          - name: docker-sock
            mountPath: /var/run/docker.sock
        - name: helm-kubectl-cli
          image: shantayya/helm-kubectl-cli
          command:
          - cat
          tty: true
        volumes:
        - name: cache
          persistentVolumeClaim:
              claimName: maven-cache
        - name: docker-sock
          hostpath: 
             path: /var/run/docker.sock
       '''}
     }
     environment{
	NEXUS_PROTOCOL='http'
	NEXUS_VERSION='nexus3'
	NEXUS_URL='IP:8081'
	NEXUS_REPOSITORY='maven-hosted'
	NEXUS_CREDENTIAL_ID='nexus-creds'
	DOCKERHUB_USER='shantayya'
	APP_NAME='petclinic'
	IMAGE_NAME="${DOCKERHUB_USER}"+ "/" + "${APP_NAME}"
	IMAGE_TAG=$BUILD_NUMBER
         }

    stages{
        stage('Checkout SCM'){
	    when { expression { true } }
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Build'){
	     when { expression { true } }
             steps{
                 container('maven'){
                     sh 'mvn clean -DskipTests package'
                             }
                    }
		post{
                     JUnit '**/target/reports/*.xml'
                     }
              }
         }
        stage('Sonar scan'){
           when { expression { true } }
           steps{
                container('sonar-cli'){
                 withSonarQubeEnv(credentialsId: 'sonar', installationName: 'sonarserver'){	--> withSonarQubeEnv() wrapper used to send scan details with server
                     sh ''' /opt/sonar-scanner/bin/sonar-scanner \  			--> execute sonar-scanner binary
			-Dsonar.projectKey=petclinic \
			-Dsonar.projectName=petcliic \
			-Dsonar.projectVersion=1.0 \
			-Dsonar.sources=src/main \
			-Dsonar.tests=src/test \
			-Dsonar.java.binaries=target/classes \
			-Dsonar.language=java \
			-Dsonar.sourceEncoding=UTF8 \
			-Dsonar.java.libraries=target/classes 
                       '''
                     }
                   }
                }
             }
          stage('Wait for quality gate'){
              when { expression { true } }
              steps{
                      container('sonar-cli'){
                         timeout(time: 1, unit: 'HOURS'){			--> create sonarqube-webhook in sonarqube so that it will send QG info to jenkins
                            waitForQualityGate abortPipeline: true		--> wait for quality get information to reach jenkins if not then abort pipeline
                            }
                          }
                    }
                }
          stage('push maven to Nexus'){
	       when { expression { false } }
               steps{
                    container('jnlp'){
                          script{
				pom = readMavenPom file: "pom.xml";
				filesByGlob = findFiles(glob:"target/*.${pom.packaging}");
				artifactPath = filesByGlob[0].path;
				artifactExists = fileExists artifactPath;
				if (artifactExists){
					nexusArtifactUploader(
						nexusVersion: NEXUS_VERSION,
						protocol: NEXUS_PROTOCOL,
						nexusUrl: NEXUS_URL,
						groupId: pom.groupId,
						version: pom.version,
						repository: NEXUS_REPOSITORY,
						credential_Id: NEXUS_CREDENTIAL_ID,
						artifacts: [
						[artifactId: pom.artifactId,
						 classifiers: '',
						 file: artifactPath,
						 type: pom.packaging],
						
						[artifactId: pom.atifactId,
						 classifiers: '',
						 file: "pom.xml",
						 type: "pom"]
                                              ]
					);
						
				}else{
					echo "***FILE: ${artifactpath} could not be found";
				   }

                              }  
                          }
                       }
                  }
            stage('Push maven to nexus through CURL'){
	   	   when { expression { true } }
                   steps{
                        container('curl'){
                          script{
				withCredentials([usernamePassword(credentialsId:'nexus-creds', passwordVariable:'pass', userVariable:'usr')]){
					pom = readMavenPom file: "pom.xml";
					sh "curl -v -u $usr:$pass --upload-file target/${pom.artifactId}-${pom.version}-${pom.packaging} \
					http://$NEXUS_URL/repository/$NEXUS_REPOSITORY/org/springframework/samples/${pom.artifactId}/${pom.version}/							${pom.artifactId}-${pom.version}-${pom.packaging}"
						
					}
			        }
                             }
                        }
                    }

	     stage('Docker build image'){
                   steps{
                        container('docker'){
					sh 'docker build -t $IMAGE_NAME:$IMAGE_TAG .'
					sh 'docker tag $IMAGE_NAME/$IMAGE_TAG $IMAGE_NAME:latest'
					withCredentials([usernamePassword(crdentialsId:'dockerhub', passwordVariable:'pass', userVariable:'user')]){
						sh 'docker login -u $user -p $pass'
						sh 'docker push $IMAGE_NAME/$IMAGE_TAG'
						sh 'docker push $IMAGE_NAME/latest'
                                         }
					sh 'docker rmi $IMAGE_NAME/$IMAGE_TAG'
					sh 'docker rmi $IMAGE_NAME/latest'
                                      }
                         }
                }
          }
            stage('Deploy to kubernetes'){
                steps{
                      container('helm-kubectl-cli'){
			   script{
				withKubeConfig([credentialsId='do_kc']){		--> withKubeConfig() wrapper is scripted syntax so used in script block
   	           		sh 'kubectl deploy -f deployment.yaml'
        	    		}
			    }
		 	  }
		     }
                } 
   }



--------------------------Jenkins Shared libraries---------------------
Jenkins shared libraries enable you to define and organize common code, functions, and steps that can be reused across multiple Jenkins pipelines. 
Jenkins says the put the repetative code(function or method) as library in jenkins repo and use function referrence in stage for reusability of pipeline as code. It helps to standardise the pipeline and need to introduce minimul code when writing new pipeline.  

How to implement shared library:
1. Assume repo structure 
jenkins-repo
|-- vars
|   -- helloWorld.groovy
|-- ...

2. create shared library under vars folder in github repository.
jenkins --> repo name
vars/helloWorld.groovy file create
def call(){
 sh 'This is shared library'
 }

3. manage jenkins --> configure system --> Global pipeline library --> add shared library name(which to be referred in pipelines) and git repo
4. import shared library in pipeline 
@Library('shared_library_name') _			--> here space _ shows all functions under library

5. call library_name() from vars/ folder in stage.
stage{
 steps{
    helloWorld()			--> file name not the function inside file. 
  }
}





-------------------------------------Interview Questions---------------------------------
1Q. Whats the difference between Matrix based security and RBAC?
Matrix-based security and Role-Based Access Control (RBAC) in Jenkins serve similar purposes but have some differences in terms of implementation and granularity of access control.

Matrix-Based Security is more focused on specifying permissions per user/group per job, lacking the role abstraction that RBAC provides.
Challenges with Matrix-Based Security:
Scalability: As the number of teams, projects, and permissions increases, managing the matrix for each job might become complex.
Hierarchy: Achieving hierarchical roles, especially when dealing with sub-teams or nuanced permission structures, can be challenging.
Group Maintenance: Ensure proper management of group memberships, especially as teams change or when new users join or leave.

Scenarios that might be challenging to achieve with Matrix-Based Security alone. 
You want to implement a role-based access control structure with different permission levels for different teams across multiple projects. Each team should have a role defining their access to jobs, views, and other Jenkins resources.

The ability to create roles, define permissions, and structure access based on projects and teams makes RBAC a powerful choice for complex Jenkins environments.


2Q. Can you explain the CICD process in your current project ? or Can you talk about any CICD process that you have implemented ?
In the current project we have been using tools orchestrated with Jenkins to achieve CICD.
Maven, SonarQube, Docker, Trivy, Nexus and Kubernetes
Coming to the implementation, the entire process takes place in 8 steps
1. Code Commit: Developers commit code changes to a Git repository hosted on GitHub.
2. Jenkins Build: Job is triggered to build the code using Maven. Maven builds the code and runs unit tests.
3. Code Analysis: Sonar is used to perform static code analysis to identify any code quality issues, security vulnerabilities, and bugs.
4. Security Scan: AppScan is used to perform a security scan on the application to identify any security vulnerabilities.
5. Deploy to Dev Environment: If the build and scans pass, Jenkins deploys the code to a development environment managed by Kubernetes.
6. Continuous Deployment: ArgoCD is used to manage continuous deployment. ArgoCD watches the Git repository and automatically deploys new changes to the development environment as soon as they are committed.
7. Promote to Production: When the code is ready for production, it is manually promoted using ArgoCD to the production environment.
8. Monitoring: The application is monitored for performance and availability using Kubernetes tools and other monitoring tools.


3Q. What are the different ways to trigger jenkins pipelines ?
This can be done in multiple ways, To briefly explain about options,
1. Trigger builds remotely (from script)
2. Build after other projects are build (upstream and downstream jobs)
3. Build periodically  (cronjob)
4. GitHub hook trigger for GitSCM polling
5. Poll SCM (cronjob) but build only if any SCM changes. 

4Q. How to backup Jenkins ?
Basically 2 ways we can backup jenkins
1. if jenkins running as service then taken snapshot of the ebs volume or use rsync to backup /var/lib/jenkins dir (we can exclude workspace dir) to avoid copying unnecessary files 
2. if jenkins is running as K8s pod then we can backup the mounted PV (take snapshot of volume) or tools like Velero or Kasten K10 are commonly used for Kubernetes backups.

One can schedule the backups to occur regularly, such as daily or weekly, to ensure that you always have a recent copy of your Jenkins environment available.


5Q. How can we restore the job configuration or version control it?
We can version-control your Jenkins configuration using the Jenkins Job Configuration History plugin.


6Q. How do you store/secure/handle secrets in Jenkins ?
there are multiple ways to achieve this, Let me give you a brief explanation of all the posible options,
   1. Credentials Plugin: Jenkins provides a credentials plugin that can be used to store secrets such as username and passwords, ssh with username and private 	key,certificates, secret text and secret file. 

   2. Environment Variables: Secrets can be stored as environment variables in Jenkins and referenced in build scripts. However, this method is less secure because 	environment variables are visible in the build logs.
   
   3. Hashicorp Vault: Jenkins can be integrated with Hashicorp Vault, which is a secure secrets management tool. Vault can be used to store and manage sensitive	 information, and Jenkins can retrieve the secrets as needed for builds.
   
   4. Third-party Secret Management Tools: Jenkins can also be integrated with third-party secret management tools such as AWS Secrets Manager, Google Cloud Key 	Management Service, and Azure Key Vault.

7Q. What is shared modules in Jenkins ?
Shared modules in Jenkins refer to a collection of reusable code and resources that can be shared across multiple Jenkins jobs. This allows for easier maintenance, reduced duplication, and improved consistency across multiple build processes. For example, shared modules can be used in cases like:
1. Libraries: Custom Java libraries, shell scripts, and other resources that can be reused across multiple jobs.
2. Jenkinsfile: A shared Jenkinsfile can be used to define the build process for multiple jobs, reducing duplication and making it easier to manage the build process for multiple projects.
3. Plugins: Common plugins can be installed once as a shared module and reused across multiple jobs, reducing the overhead of managing plugins on individual jobs.
4. Global Variables: Shared global variables can be defined and used across multiple jobs, making it easier to manage common build parameters such as version numbers, artifact repositories, and environment variables.


8Q. Can you use Jenkins to build applications with multiple programming languages using different agents in different stages ?
Yes, Jenkins can be used to build applications with multiple programming languages by using different build agents in different stages of the build process.
Jenkins supports multiple build agents, which can be used to run build jobs on different platforms and with different configurations.


9Q.  How to setup auto-scaling group for Jenkins in AWS ?
Here is a high-level overview of how to set up an autoscaling group for Jenkins in Amazon Web Services (AWS):
1. Launch EC2 instances: Create an EC2 instance with the desired configuration and install Jenkins on it. This instance will be used as the base image for the autoscaling group.
2. Create Launch Configuration: Create a launch configuration in AWS Auto Scaling that specifies the EC2 instance type, the base image (created in step 1), and any additional configuration settings such as storage, security groups, and key pairs.
3. Create Autoscaling Group: Create an autoscaling group in AWS Auto Scaling and specify the launch configuration created in step 2. Also, specify the desired number of instances, the minimum number of instances, and the maximum number of instances for the autoscaling group.
4. Configure Scaling Policy: Configure a scaling policy for the autoscaling group to determine when new instances should be added or removed from the group. This can be based on the average CPU utilization of the instances or other performance metrics.
5. Load Balancer: Create a load balancer in ELB and configure it to forward traffic to the autoscaling group.
6. Connect to Jenkins: Connect to the Jenkins instance using the load balancer endpoint or the public IP address of one of the instances in the autoscaling group.
7. Monitoring: Monitor the instances in the autoscaling group using Amazon CloudWatch to ensure that they are healthy and that the autoscaling policy is functioning as expected.

 By using an autoscaling group for Jenkins, you can ensure that you have the appropriate number of instances available to handle the load on your build processes, and that new instances can be added or removed automatically as needed. This helps to ensure the reliability and scalability of your Jenkins environment.

Note: Dynamic Agent Provisioning:
Jenkins supports plugins like the EC2 Plugin or Amazon EC2 Container Service (ECS) Plugin, which allow for dynamic provisioning of agents in AWS, including within an ASG. These plugins can automatically provision and terminate agents based on the current load, and they often use AWS credentials and settings to launch instances from a specified AMI.

10Q. How to add a new worker node in Jenkins ?
Log into the Jenkins master and navigate to Manage Jenkins > Manage Nodes > New Node. Enter a name for the new node and select Permanent Agent. Configure SSH and click on Launch.


11Q. How to add a new plugin in Jenkins ?
Using the CLI --> java -jar jenkins-cli.jar install-plugin <PLUGIN_NAME>
Using the UI --> Click on the "Manage Jenkins" --> "Manage Plugins" --> install 


12Q. What is JNLP and why is it used in Jenkins ?
Jenkins master uses JNLP(Java network launch protocol) service on port 50000 to connect to slaves for dispatching jobs.
When a Jenkins agent is launched using JNLP, it connects to the Jenkins master and receives build tasks, which it then executes. The results of the build are then sent back to the master and displayed in the Jenkins user interface.


13Q. What are some of the common plugins that you use in Jenkins ?
Role based auth strategy, git, maven, sonar-cli, nexus platform, generic-webhooc-trigger, kubernetes-cli, embeddable-build-status,post-build-steps

14. How do you upgrade Jenkins?
15. What is called a Parameterised Job in Jenkins ?
16. How do you manage space issues in the Jenkins server?
17. what is called a multibranch project in the Jenkins server ?
18. How do you secure the Jenkins server?
19. What is called Jenkins File?
20. How do you configure the job in Jenkins?
21. Where do you find errors in Jenkins?
22. How do you integrate sonar Qube in Jenkins?
23. In Jenkins how can you find log files?
24. Jenkins workflow and write a script for this workflow.
25. Write a script for how to push the repository and build the job in Jenkins.
26. How to build a job in Jenkins?
27. How to create a continuous deployment in Jenkins?
28. In Jenkins how to give backup from one server to another server?
29. Why do we use a pipeline in Jenkins? Flow?
30. Say your jenkins jobs were deleted mistakenly and you want to recover all jobs so how do you achieve it? jenkins casc
31. How do you configure ansible in Jenkins?

please give me answers for above interview questions as per your knowledge. I dont want theorotical answer please